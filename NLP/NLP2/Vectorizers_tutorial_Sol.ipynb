{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words\n",
    "We need to convert the string representation of the corpus into a numeric representation that we can apply our machine learning algorithms to. We discard most of the structure of the input text, and only count how oftern each word appears in each text in the corpus. This is the mental image of a \"bag of words\" for a corpus of docs. It consists of three steps:\n",
    "\n",
    "- Tokenization, (CountVectorizer)\n",
    "- Vocabulary Building, (fitting the CountVectorizer builds the vocabulary)(vocabulary_)\n",
    "- Encoding (in the form of SciPy sparse matrix)(boW is created via transform) (one vector of word counts for each document in the corpus)(for each word in the document, we have a count of how often it appears in each document)\n",
    "\n",
    "CountVectorizer eliminates single letter words like \"a\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a Count Vectorizer, we simply need to instantiate one. We are not using any parameters yet.\n",
    "\n",
    "Fitting of the CountVectorizer consists of tokenization of the training data, and building the vocabulary. Transforming the CountVectorizer creates the bag-of-words representation of the train data. The bow is stored in a SciPy sparse matrix that only stores the nonzero\n",
    "entries. To look at the actual content of the sparse matrix, convert it to dense array using numpy.toarray() method\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Let's get a sample text\n",
    "sample_text = [\"I was wondering if anyone out there could enlighten me on this car I saw\"\n",
    "               \"the other day. It was a 2-door sports car, looked to be from the late 60s/\"\n",
    "               \"early 70s. It was called a Bricklin. The doors were really small. In addition,\"\n",
    "              ]\n",
    "sample_text_1 = [\"One of the most basic ways we can numerically represent words \"\n",
    "               \"is through the one-hot encoding method (also sometimes called \"\n",
    "               \"count vectorizing).\"]\n",
    "sample_text_2 = ['One, one, one, we can Dance! Such a dance!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the CountVectorizer\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit call tokenizes the text, and builds the vocabulary.\n",
    "vectorizer.fit(sample_text_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the vocabulary of your text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'one': 2, 'we': 4, 'can': 0, 'dance': 1, 'such': 3}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To actually create the vectorizer, we simply need to call fit on\n",
    "# the text\n",
    "# You can get the shape after you transform\n",
    "\n",
    "\n",
    "# transform \n",
    "vectorizer.transform(sample_text_2)\n",
    "\n",
    "\n",
    "# check the shape\n",
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'one': 2, 'we': 4, 'can': 0, 'dance': 1, 'such': 3}\n"
     ]
    }
   ],
   "source": [
    "# Now, we can inspect how our vectorizer vectorized the text\n",
    "# This will print out a list of words used, and their index in the \n",
    "# vectors. As you notice, all the vocabulary is lower-cased!! Also,\n",
    "# the punctuation is left out. Single letter words is also eliminated. \n",
    "\n",
    "# see the vocabulary created for the data set\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5)\n",
      "  (0, 0)\t1\n",
      "  (0, 1)\t2\n",
      "  (0, 2)\t3\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "[[1 2 3 1 1]]\n",
      "[[1 2 3 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# If we would like to actually create a vector, we can do so by \n",
    "# passing the text into the vectorizer to get back counts\n",
    "# sparse matrix -> dense matrix (you can use toarray() or to_dense())\n",
    "vector = vectorizer.transform(sample_text_2)\n",
    "print(vector.shape)\n",
    "print(vector)\n",
    "print(vector.toarray())\n",
    "print(vector.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5)\n",
      "\n",
      "[[0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "vector2 = vectorizer.transform(sample_text)\n",
    "print(vector2.shape)\n",
    "print(vector2)\n",
    "print(vector2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0]]\n",
      "[[0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Or if we wanted to get the vector for one word:\n",
    "print(vectorizer.transform(['one']).toarray())\n",
    "# OOV (out-of-vocabulary)\n",
    "\n",
    "print(vectorizer.transform(['hot']).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That is how you get the vector for test data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's repeat the example with another text\n",
    "new_text = [' Today is the day that I do the thing today, today']\n",
    "\n",
    "#instantiate the CountVectorizer using a user-defined tokenizer\n",
    "new_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'today': 6, 'is': 2, 'the': 4, 'day': 0, 'that': 3, 'do': 1, 'thing': 5}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the vector after fit_transform\n",
    "new_vectorizer.fit_transform(new_text)\n",
    "\n",
    "# look at the vocabulary\n",
    "new_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 7)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what is the shape of the vector\n",
    "\n",
    "new_vectorizer.transform(new_text).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TfidfVectorizer, TfidfTransformer\n",
    "\n",
    "The TfidfVectorizer will tokenize documents, learn the vocabulary and inverse document frequency weights, and allow you to encode new documents. Alternately, if you already have a learned CountVectorizer, you can use it with a TfidfTransformer to just calculate the inverse document frequencies and start encoding documents.\n",
    "\n",
    "The same create, fit, and transform process is used as with the CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the TfidfVectorizer, and TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# instantiate the vectorizer\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the vectorizer for sample_text_2\n",
    "vectorizer.fit(sample_text_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'one': 2, 'we': 4, 'can': 0, 'dance': 1, 'such': 3}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observe the vocabulary\n",
    "vectorizer.vocabulary_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the vectorizer\n",
    "\n",
    "vector = vectorizer.transform(sample_text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5)\n",
      "[[0.25 0.5  0.75 0.25 0.25]]\n"
     ]
    }
   ],
   "source": [
    "# shape of the vector\n",
    "print(vector.shape)\n",
    "print(vector.todense())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.25 0.5  0.75 0.25 0.25]]\n"
     ]
    }
   ],
   "source": [
    "# if you want to see your vector, convert it to dense array\n",
    "print(vector.todense())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3 1 1]]\n",
      "{'one': 2, 'we': 4, 'can': 0, 'dance': 1, 'such': 3}\n",
      "[1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[0.25, 0.5 , 0.75, 0.25, 0.25]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#instantiate CountVectorizer() \n",
    "cv=CountVectorizer() \n",
    " \n",
    "# this steps generates word counts for the words in your docs \n",
    "word_count_vector=cv.fit_transform(sample_text_2)\n",
    "print(word_count_vector.todense())\n",
    "print(cv.vocabulary_)\n",
    "tfidf_transformer=TfidfTransformer() \n",
    "tfidf_transformer.fit(word_count_vector)\n",
    "print(tfidf_transformer.idf_)\n",
    "tfidf_transformer.transform(word_count_vector).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively, you can use toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see another text\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\",\n",
    "        \"The dog.\",\n",
    "        \"The fox\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the vectorizer\n",
    "vectorizer.fit(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 7,\n",
       " 'quick': 6,\n",
       " 'brown': 0,\n",
       " 'fox': 2,\n",
       " 'jumped': 3,\n",
       " 'over': 5,\n",
       " 'lazy': 4,\n",
       " 'dog': 1}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observe the vocabulary\n",
    "\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.69314718, 1.28768207, 1.28768207, 1.69314718, 1.69314718,\n",
       "       1.69314718, 1.69314718, 1.        ])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can see the idf values with .idf_\n",
    "\n",
    "vectorizer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jumped over the lazy dog.\n"
     ]
    }
   ],
   "source": [
    "# transform the vectorizer for the first document. \n",
    "print(text[0])\n",
    "vector=vectorizer.transform([text[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of the vector \n",
    "vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36388646, 0.27674503, 0.27674503, 0.36388646, 0.36388646,\n",
       "        0.36388646, 0.36388646, 0.42983441]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the vector by converting to dense array\n",
    "vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see the vector in a df format\n",
    "# vectorizer.get_feature_names() returns your features\n",
    "import pandas as pd\n",
    "vectorizer = TfidfVectorizer()\n",
    "vector = vectorizer.fit_transform(text)\n",
    "feature_name = vectorizer.get_feature_names()\n",
    "\n",
    "dense = vector.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brown</th>\n",
       "      <th>dog</th>\n",
       "      <th>fox</th>\n",
       "      <th>jumped</th>\n",
       "      <th>lazy</th>\n",
       "      <th>over</th>\n",
       "      <th>quick</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.363886</td>\n",
       "      <td>0.276745</td>\n",
       "      <td>0.276745</td>\n",
       "      <td>0.363886</td>\n",
       "      <td>0.363886</td>\n",
       "      <td>0.363886</td>\n",
       "      <td>0.363886</td>\n",
       "      <td>0.429834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.789807</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.613356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.789807</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.613356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      brown       dog       fox    jumped      lazy      over     quick  \\\n",
       "0  0.363886  0.276745  0.276745  0.363886  0.363886  0.363886  0.363886   \n",
       "1  0.000000  0.789807  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.789807  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "        the  \n",
       "0  0.429834  \n",
       "1  0.613356  \n",
       "2  0.613356  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at your dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# min_df\n",
    "CountVectorizer tokenizes using a regular expression \"\\b\\w\\w+\\b\". It also converts all to lowercase letters. \n",
    "\n",
    "**Note:** \\w\\w+ translates to [a-zA-Z0-9_][a-zA-Z0-9_]+ (which can be written as [a-zA-Z0-9_]{2,}). This matches 2 or more alphanumeric characters (as defined between the square brackets).\n",
    "\n",
    "The \\b matches word boundaries: anything that is not an alphanumeric character, next to something alphanumeric. This includes spaces and punctuation, so it also includes the dot and causes the separation.\n",
    "\n",
    "# Stop words\n",
    "Fixed sets of stopwords are not very likely to increase the performance. Fixed sets are usually good for small datasets. So, let's try another approach. Removing stopwords might not always be a good idea especially in sentiment analysis. By removing the stop word, a negative rating could turn out to be positive and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HashingVectorizer\n",
    "Note that this vectorizer does not require a call to fit on the training data documents. Instead, after instantiation, it can be used directly to start encoding documents.\n",
    "\n",
    "The values of the encoded document correspond to normalized word counts by default in the range of -1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "# list of text docs\n",
    "sample = [\"The quick brown fox jumped over the lazy dog.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the transform with 5 features\n",
    "\n",
    "hvectorizer = HashingVectorizer(n_features = 10, norm=None, alternate_sign=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the doc\n",
    "hvector = hvectorizer.fit_transform(sample)\n",
    "\n",
    "#print(vector.vocabulary_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The quick brown fox jumped over the lazy dog.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the shape of your vector\n",
    "\n",
    "#print(dir(vector))\n",
    "hvector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 1., 0., 2., 0., 2., 0., 1., 3., 0.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hvector.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1.0\n",
      "  (0, 3)\t2.0\n",
      "  (0, 5)\t2.0\n",
      "  (0, 7)\t1.0\n",
      "  (0, 8)\t3.0\n"
     ]
    }
   ],
   "source": [
    "# print populated columns of first document\n",
    "# format: (doc id, pos_in_matrix)  raw_count\n",
    "print(hvector[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7)\t2\n",
      "  (0, 6)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 1)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cvectorizer = CountVectorizer()\n",
    "\n",
    "# compute counts without any term frequency normalization\n",
    "X = cvectorizer.fit_transform(sample)\n",
    "\n",
    "# print populated columns of first document\n",
    "# format: (doc id, pos_in_matrix)  raw_count\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 11)\t1.0\n",
      "  (0, 15)\t1.0\n",
      "  (0, 53)\t1.0\n",
      "  (0, 58)\t2.0\n",
      "  (0, 85)\t1.0\n",
      "  (0, 87)\t1.0\n",
      "  (0, 88)\t1.0\n",
      "  (0, 93)\t1.0\n"
     ]
    }
   ],
   "source": [
    "hvectorizer = HashingVectorizer(n_features=100,norm=None,alternate_sign=False) \n",
    "\n",
    "# compute counts without any term frequency normalization \n",
    "X = hvectorizer.fit_transform(sample)\n",
    "# print populated columns of first document\n",
    "# format: (doc id, pos_in_matrix)  raw_count\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
