{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Example 1\n",
    "For this example, we have images of cars and flowers, which have been divided into training and testing sets, and we have to build a CNN that identifies whether an image is a car or a flower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import the numpy library and the necessary Keras libraries and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "from tensorflow import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Now, set a seed and initiate the model with the `Sequential` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set a seed\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "random.set_seed(seed)\n",
    "\n",
    "# Initialising the CNN\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Add the first layer of the CNN, set the input shape to (64, 64, 3), the dimension of each image, and set the activation function as a ReLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BalintStewart\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "input_shape = (64, 64, 3) # 3 corresponds to number of channels eg RGB\n",
    "# keep the filter size as default\n",
    "classifier.add(Conv2D(filters = 32, kernel_size = (3,3), input_shape= input_shape, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Now, add the pooling layer with the image size as 2x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(MaxPool2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Flatten the output of the pooling layer by adding a flattening layer to the CNN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Add the first Dense layer of the MLP. \n",
    "Here, 128 is the output of the number of nodes. As a good practice, 128 is good to get started. activation is relu. As a good practice, the power of two is preferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(128, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Add the output layer of the MLP.\n",
    "This is a binary classification problem, so the size is 1 and the activation is `sigmoid`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30752</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,936,384</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_3 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30752\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m3,936,384\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,937,409</span> (15.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,937,409\u001b[0m (15.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,937,409</span> (15.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,937,409\u001b[0m (15.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier.add(Dense(1, activation = 'sigmoid'))\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Compile the network\n",
    "Use an adam optimizer and compute the accuracy during the training process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer= 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Create training and test data generators. \n",
    "- Rescale the training and test images by `1/255` so that all the values are between `0` and `1`.\n",
    "- Set these parameters for the training data generators only \n",
    " - `shear_range=0.2`, `zoom_range=0.2`, and `horizontal_flip=True`\n",
    " \n",
    " - https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# For test data generator, only use rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Create a training set from the training set folder.\n",
    "'training_set' is the folder where our data has been placed. Our CNN model has an image size of `64x64`, so the same size should be passed here too. `batch_size` is the number of images in a single batch, which is `32`. `Class_mode` is set to binary since we are working on binary classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Define paths\n",
    "base_dir = \"car_flower_small\"\n",
    "train_dir = \"car_flower_train\"\n",
    "test_dir = \"car_flower_test\"\n",
    "split_ratio = 0.8  # try an 80% split, do a meanual split\n",
    "\n",
    "# Create directories for train and test sets\n",
    "for category in ['car', 'flower']:\n",
    "    os.makedirs(os.path.join(train_dir, category), exist_ok=True)\n",
    "    os.makedirs(os.path.join(test_dir, category), exist_ok=True)\n",
    "\n",
    "# group files by label class\n",
    "all_files = os.listdir(base_dir)\n",
    "car_files = [f for f in all_files if f.startswith('car')]\n",
    "flower_files = [f for f in all_files if f.startswith('flower')]\n",
    "\n",
    "# Function to split and copy files\n",
    "def split_and_copy(files, category):\n",
    "    random.shuffle(files)\n",
    "    split_point = int(len(files) * split_ratio)\n",
    "    train_files = files[:split_point]\n",
    "    test_files = files[split_point:]\n",
    "\n",
    "    for file in train_files:\n",
    "        shutil.copy(os.path.join(base_dir, file), os.path.join(train_dir, category, file))\n",
    "\n",
    "    for file in test_files:\n",
    "        shutil.copy(os.path.join(base_dir, file), os.path.join(test_dir, category, file))\n",
    "\n",
    "# Split and copy data - only run this when you want to create more copies \n",
    "\n",
    "split_and_copy(car_files, 'car')\n",
    "split_and_copy(flower_files, 'flower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1597 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# paths to the train and test directories\n",
    "train_data_dir = \"car_flower_train\"\n",
    "test_data_dir = \"car_flower_test\"\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,  # this is the target directory\n",
    "        target_size=(64, 64),  # see the instructions above\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Repeat step 10 for the test set \n",
    "while setting the folder to the location of the test images, that is, 'test_set'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_directory( # test_datagen only does the rescaling\n",
    "        test_data_dir,  # this is the target directory\n",
    "        target_size=(64, 64),  # see the instructions above\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split across training and test data looks good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12: Finally, fit the data. \n",
    "Set the `steps_per_epoch` to `STEP_SIZE_TRAIN` and the `validation_steps` to `STEP_SIZE_TEST`. \n",
    "\n",
    "Why do we need `steps_per_epoch` ?\n",
    "\n",
    "Keep in mind that a Keras data generator is meant to loop infinitely — it should never return or exit.\n",
    "\n",
    "Since the function is intended to loop infinitely, Keras has no ability to determine when one epoch starts and a new epoch begins.\n",
    "\n",
    "Therefore, we compute the `steps_per_epoch` value as the total number of training data points divided by the batch size. Once Keras hits this step count it knows that it’s a new epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 74ms/step - accuracy: 0.7773 - loss: 0.4623 - val_accuracy: 0.7656 - val_loss: 0.5422\n",
      "Epoch 2/50\n",
      "\u001b[1m 1/49\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.8750 - loss: 0.4156"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BalintStewart\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8750 - loss: 0.4156 - val_accuracy: 0.7630 - val_loss: 0.5267\n",
      "Epoch 3/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 79ms/step - accuracy: 0.8135 - loss: 0.4173 - val_accuracy: 0.7630 - val_loss: 0.5318\n",
      "Epoch 4/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7812 - loss: 0.4711 - val_accuracy: 0.7578 - val_loss: 0.5159\n",
      "Epoch 5/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 89ms/step - accuracy: 0.8090 - loss: 0.4186 - val_accuracy: 0.7656 - val_loss: 0.4956\n",
      "Epoch 6/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8750 - loss: 0.3188 - val_accuracy: 0.7839 - val_loss: 0.4754\n",
      "Epoch 7/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 107ms/step - accuracy: 0.8322 - loss: 0.3795 - val_accuracy: 0.7708 - val_loss: 0.4934\n",
      "Epoch 8/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7812 - loss: 0.3491 - val_accuracy: 0.7578 - val_loss: 0.5141\n",
      "Epoch 9/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 113ms/step - accuracy: 0.8066 - loss: 0.4146 - val_accuracy: 0.7865 - val_loss: 0.5139\n",
      "Epoch 10/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9688 - loss: 0.2568 - val_accuracy: 0.7682 - val_loss: 0.5318\n",
      "Epoch 11/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 103ms/step - accuracy: 0.8476 - loss: 0.3546 - val_accuracy: 0.7865 - val_loss: 0.5267\n",
      "Epoch 12/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7500 - loss: 0.4133 - val_accuracy: 0.7839 - val_loss: 0.5301\n",
      "Epoch 13/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 120ms/step - accuracy: 0.8624 - loss: 0.3284 - val_accuracy: 0.7839 - val_loss: 0.4669\n",
      "Epoch 14/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8125 - loss: 0.3737 - val_accuracy: 0.7865 - val_loss: 0.4684\n",
      "Epoch 15/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 124ms/step - accuracy: 0.8540 - loss: 0.3412 - val_accuracy: 0.7969 - val_loss: 0.5032\n",
      "Epoch 16/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8750 - loss: 0.2662 - val_accuracy: 0.7891 - val_loss: 0.5193\n",
      "Epoch 17/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.8758 - loss: 0.3282 - val_accuracy: 0.7995 - val_loss: 0.5282\n",
      "Epoch 18/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8125 - loss: 0.4107 - val_accuracy: 0.7995 - val_loss: 0.5200\n",
      "Epoch 19/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 110ms/step - accuracy: 0.8925 - loss: 0.2831 - val_accuracy: 0.7708 - val_loss: 0.6196\n",
      "Epoch 20/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.1251 - val_accuracy: 0.7604 - val_loss: 0.6549\n",
      "Epoch 21/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 116ms/step - accuracy: 0.8525 - loss: 0.3299 - val_accuracy: 0.7865 - val_loss: 0.5038\n",
      "Epoch 22/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7812 - loss: 0.4122 - val_accuracy: 0.7839 - val_loss: 0.5103\n",
      "Epoch 23/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 123ms/step - accuracy: 0.8809 - loss: 0.2761 - val_accuracy: 0.7969 - val_loss: 0.5634\n",
      "Epoch 24/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7812 - loss: 0.4061 - val_accuracy: 0.7943 - val_loss: 0.5700\n",
      "Epoch 25/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.8981 - loss: 0.2578 - val_accuracy: 0.8073 - val_loss: 0.5162\n",
      "Epoch 26/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9062 - loss: 0.2193 - val_accuracy: 0.7995 - val_loss: 0.5543\n",
      "Epoch 27/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 107ms/step - accuracy: 0.8924 - loss: 0.2554 - val_accuracy: 0.7995 - val_loss: 0.5276\n",
      "Epoch 28/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9375 - loss: 0.2275 - val_accuracy: 0.8073 - val_loss: 0.4869\n",
      "Epoch 29/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 117ms/step - accuracy: 0.8811 - loss: 0.2874 - val_accuracy: 0.7865 - val_loss: 0.4866\n",
      "Epoch 30/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9062 - loss: 0.2309 - val_accuracy: 0.7760 - val_loss: 0.5623\n",
      "Epoch 31/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.8887 - loss: 0.2686 - val_accuracy: 0.7995 - val_loss: 0.5216\n",
      "Epoch 32/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8750 - loss: 0.3583 - val_accuracy: 0.7969 - val_loss: 0.5397\n",
      "Epoch 33/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 106ms/step - accuracy: 0.9019 - loss: 0.2434 - val_accuracy: 0.7969 - val_loss: 0.4948\n",
      "Epoch 34/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9062 - loss: 0.2374 - val_accuracy: 0.7943 - val_loss: 0.4712\n",
      "Epoch 35/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 114ms/step - accuracy: 0.9046 - loss: 0.2283 - val_accuracy: 0.7969 - val_loss: 0.6816\n",
      "Epoch 36/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8276 - loss: 0.3738 - val_accuracy: 0.8099 - val_loss: 0.5959\n",
      "Epoch 37/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.9033 - loss: 0.2381 - val_accuracy: 0.7891 - val_loss: 0.5626\n",
      "Epoch 38/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9062 - loss: 0.2151 - val_accuracy: 0.7943 - val_loss: 0.5667\n",
      "Epoch 39/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 115ms/step - accuracy: 0.8990 - loss: 0.2640 - val_accuracy: 0.8099 - val_loss: 0.4934\n",
      "Epoch 40/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9375 - loss: 0.1983 - val_accuracy: 0.7969 - val_loss: 0.5059\n",
      "Epoch 41/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 120ms/step - accuracy: 0.9183 - loss: 0.2117 - val_accuracy: 0.8203 - val_loss: 0.4950\n",
      "Epoch 42/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.1143 - val_accuracy: 0.8255 - val_loss: 0.4767\n",
      "Epoch 43/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 106ms/step - accuracy: 0.9177 - loss: 0.2036 - val_accuracy: 0.7943 - val_loss: 0.5583\n",
      "Epoch 44/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8750 - loss: 0.1773 - val_accuracy: 0.8021 - val_loss: 0.5797\n",
      "Epoch 45/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 122ms/step - accuracy: 0.9285 - loss: 0.1826 - val_accuracy: 0.8021 - val_loss: 0.5782\n",
      "Epoch 46/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9062 - loss: 0.2184 - val_accuracy: 0.8099 - val_loss: 0.5930\n",
      "Epoch 47/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 102ms/step - accuracy: 0.9176 - loss: 0.1972 - val_accuracy: 0.8021 - val_loss: 0.5965\n",
      "Epoch 48/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9062 - loss: 0.2630 - val_accuracy: 0.8229 - val_loss: 0.5137\n",
      "Epoch 49/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 112ms/step - accuracy: 0.9224 - loss: 0.1760 - val_accuracy: 0.8125 - val_loss: 0.5265\n",
      "Epoch 50/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9062 - loss: 0.1787 - val_accuracy: 0.7969 - val_loss: 0.5002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2c8fd1e1730>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STEP_SIZE_TRAIN = 1597 // batch_size\n",
    "STEP_SIZE_TEST = 400 // batch_size\n",
    "\n",
    "classifier.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch = STEP_SIZE_TRAIN,\n",
    "    epochs = 50,\n",
    "    validation_data = test_generator,\n",
    "    validation_steps = 400 // batch_size\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
