{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "import json\n",
    "# For data splitting\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "# For scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# For modeling\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "\n",
    "# For evaluation\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# For pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# For GridSearch\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Import additional estimators\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/auto_scout_final.csv')\n",
    "# Create features and targey variable\n",
    "X = df.drop('price', axis = 1)\n",
    "y = df['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into test and train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard scaler\n",
    "standard_scaler = StandardScaler()\n",
    "X_train_scaled = standard_scaler.fit_transform(X_train) # train the scalar on the train dataset\n",
    "X_test_scaled = standard_scaler.transform(X_test) # transform test datasets, make sure you don't fit to test data though!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run and initial pass of all the major types of estimators (not fine-tuned) on the data and compare performance on train and test datasets. Try unscaled data first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty dict to store evaluation results\n",
    "results = {}\n",
    "\n",
    "models = {'SVR': SVR(),\n",
    "          'Random Forest': RandomForestRegressor(),\n",
    "          'Gradient Boost': GradientBoostingRegressor(),\n",
    "          'Lasso Regression': Lasso(),\n",
    "          'XG Boost': XGBRegressor()}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    #unscaled data\n",
    "    model.fit(X_train, y_train)\n",
    "    # make predictions on the training data and the test data\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    # Gather evaluation metrics mean_squared_error and r2_score on both train and test datasets\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    train_MSE = mean_squared_error(y_train, y_train_pred)\n",
    "    test_MSE = mean_squared_error(y_test, y_test_pred)\n",
    "    train_RMSE = np.sqrt(train_MSE)\n",
    "    test_RMSE = np.sqrt(test_MSE)\n",
    "    # put these results into my results dict\n",
    "    results[model_name] = {\n",
    "        'Train R^2':train_r2,\n",
    "        'Test R^2':test_r2,\n",
    "        'Train RMSE':train_RMSE,\n",
    "        'Test RMSE': test_RMSE\n",
    "    }\n",
    "\n",
    "# convert dict into df\n",
    "results_unscaled = pd.DataFrame(results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train R^2</th>\n",
       "      <th>Test R^2</th>\n",
       "      <th>Train RMSE</th>\n",
       "      <th>Test RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVR</th>\n",
       "      <td>0.055281</td>\n",
       "      <td>0.062925</td>\n",
       "      <td>7252.321013</td>\n",
       "      <td>6904.640536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.993551</td>\n",
       "      <td>0.952596</td>\n",
       "      <td>599.180517</td>\n",
       "      <td>1552.966249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost</th>\n",
       "      <td>0.941567</td>\n",
       "      <td>0.929793</td>\n",
       "      <td>1803.667987</td>\n",
       "      <td>1889.925405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso Regression</th>\n",
       "      <td>0.899707</td>\n",
       "      <td>0.890865</td>\n",
       "      <td>2362.981372</td>\n",
       "      <td>2356.329187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XG Boost</th>\n",
       "      <td>0.987915</td>\n",
       "      <td>0.956820</td>\n",
       "      <td>820.251350</td>\n",
       "      <td>1482.156908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Train R^2  Test R^2   Train RMSE    Test RMSE\n",
       "SVR                0.055281  0.062925  7252.321013  6904.640536\n",
       "Random Forest      0.993551  0.952596   599.180517  1552.966249\n",
       "Gradient Boost     0.941567  0.929793  1803.667987  1889.925405\n",
       "Lasso Regression   0.899707  0.890865  2362.981372  2356.329187\n",
       "XG Boost           0.987915  0.956820   820.251350  1482.156908"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_unscaled.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compare performance on the scaled data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty dict to store evaluation results\n",
    "results = {}\n",
    "\n",
    "models = {'SVR': SVR(),\n",
    "          'Random Forest': RandomForestRegressor(),\n",
    "          'Gradient Boost': GradientBoostingRegressor(),\n",
    "          'Lasso Regression': Lasso(),\n",
    "          'XG Boost': XGBRegressor()}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    #unscaled data\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    # make predictions on the training data and the test data\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "    # Gather evaluation metrics mean_squared_error and r2_score on both train and test datasets\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    train_MSE = mean_squared_error(y_train, y_train_pred)\n",
    "    test_MSE = mean_squared_error(y_test, y_test_pred)\n",
    "    train_RMSE = np.sqrt(train_MSE)\n",
    "    test_RMSE = np.sqrt(test_MSE)\n",
    "    # put these results into my results dict\n",
    "    results[model_name] = {\n",
    "        'Train R^2 scaled':train_r2,\n",
    "        'Test R^2 scaled':test_r2,\n",
    "        'Train RMSE scaled':train_RMSE,\n",
    "        'Test RMSE scaled': test_RMSE\n",
    "    }\n",
    "\n",
    "# convert dict into df\n",
    "results_scaled = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gradient Boost</th>\n",
       "      <th>Lasso Regression</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>SVR</th>\n",
       "      <th>XG Boost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train R^2</th>\n",
       "      <td>0.941567</td>\n",
       "      <td>0.899707</td>\n",
       "      <td>0.993551</td>\n",
       "      <td>0.055281</td>\n",
       "      <td>0.987915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test R^2</th>\n",
       "      <td>0.929793</td>\n",
       "      <td>0.890865</td>\n",
       "      <td>0.952596</td>\n",
       "      <td>0.062925</td>\n",
       "      <td>0.956820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train RMSE</th>\n",
       "      <td>1803.667987</td>\n",
       "      <td>2362.981372</td>\n",
       "      <td>599.180517</td>\n",
       "      <td>7252.321013</td>\n",
       "      <td>820.251350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test RMSE</th>\n",
       "      <td>1889.925405</td>\n",
       "      <td>2356.329187</td>\n",
       "      <td>1552.966249</td>\n",
       "      <td>6904.640536</td>\n",
       "      <td>1482.156908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train R^2 scaled</th>\n",
       "      <td>0.941567</td>\n",
       "      <td>0.900008</td>\n",
       "      <td>0.993373</td>\n",
       "      <td>0.008712</td>\n",
       "      <td>0.987915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test R^2 scaled</th>\n",
       "      <td>0.929680</td>\n",
       "      <td>0.890716</td>\n",
       "      <td>0.953145</td>\n",
       "      <td>0.016612</td>\n",
       "      <td>0.956820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train RMSE scaled</th>\n",
       "      <td>1803.667987</td>\n",
       "      <td>2359.439317</td>\n",
       "      <td>607.415629</td>\n",
       "      <td>7428.918463</td>\n",
       "      <td>820.251350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test RMSE scaled</th>\n",
       "      <td>1891.445306</td>\n",
       "      <td>2357.932108</td>\n",
       "      <td>1543.944582</td>\n",
       "      <td>7073.205589</td>\n",
       "      <td>1482.156908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Gradient Boost  Lasso Regression  Random Forest  \\\n",
       "Train R^2                0.941567          0.899707       0.993551   \n",
       "Test R^2                 0.929793          0.890865       0.952596   \n",
       "Train RMSE            1803.667987       2362.981372     599.180517   \n",
       "Test RMSE             1889.925405       2356.329187    1552.966249   \n",
       "Train R^2 scaled         0.941567          0.900008       0.993373   \n",
       "Test R^2 scaled          0.929680          0.890716       0.953145   \n",
       "Train RMSE scaled     1803.667987       2359.439317     607.415629   \n",
       "Test RMSE scaled      1891.445306       2357.932108    1543.944582   \n",
       "\n",
       "                           SVR     XG Boost  \n",
       "Train R^2             0.055281     0.987915  \n",
       "Test R^2              0.062925     0.956820  \n",
       "Train RMSE         7252.321013   820.251350  \n",
       "Test RMSE          6904.640536  1482.156908  \n",
       "Train R^2 scaled      0.008712     0.987915  \n",
       "Test R^2 scaled       0.016612     0.956820  \n",
       "Train RMSE scaled  7428.918463   820.251350  \n",
       "Test RMSE scaled   7073.205589  1482.156908  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.concat([results_unscaled, results_scaled], axis = 0)\n",
    "results[sorted(results)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the data doesn't seem to affect performance across these models. XG Boost is best performing model, test RMSE = 1482, R^2 = 0.957"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log transforming the target variable improved linear regression a bit, can it improve the other models too?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transformation to the target variable\n",
    "y_train_log = np.log(y_train)\n",
    "y_test_log = np.log(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty dict to store evaluation results\n",
    "results = {}\n",
    "\n",
    "models = {'SVR': SVR(),\n",
    "          'Random Forest': RandomForestRegressor(),\n",
    "          'Gradient Boost': GradientBoostingRegressor(),\n",
    "          'Lasso Regression': Lasso(),\n",
    "          'XG Boost': XGBRegressor()}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    #unscaled data\n",
    "    model.fit(X_train, y_train_log)\n",
    "    # make predictions on the training data and the test data\n",
    "    y_train_pred_log = model.predict(X_train)\n",
    "    y_test_pred_log = model.predict(X_test)\n",
    "    # Inverse log transform the predictions and the true values to original scale, so can compare RMSE values across log and non-transformed target variable\n",
    "    y_train_pred_original = np.exp(y_train_pred_log)\n",
    "    y_test_pred_original = np.exp(y_test_pred_log)\n",
    "    y_train_original = np.exp(y_train_log)\n",
    "    y_test_original = np.exp(y_test_log)\n",
    "    # Gather evaluation metrics mean_squared_error and r2_score on both train and test datasets\n",
    "    train_r2 = r2_score(y_train_log, y_train_pred_log)\n",
    "    test_r2 = r2_score(y_test_log, y_test_pred_log)\n",
    "    train_MSE = mean_squared_error(y_train_original, y_train_pred_original)\n",
    "    test_MSE = mean_squared_error(y_test_original, y_test_pred_original)\n",
    "    train_RMSE = np.sqrt(train_MSE)\n",
    "    test_RMSE = np.sqrt(test_MSE)\n",
    "    # put these results into my results dict\n",
    "    results[model_name] = {\n",
    "        'Train R^2 log':train_r2,\n",
    "        'Test R^2 log':test_r2,\n",
    "        'Train RMSE log':train_RMSE,\n",
    "        'Test RMSE log': test_RMSE\n",
    "    }\n",
    "\n",
    "# convert dict into df\n",
    "results_logTarget = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SVR</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>Gradient Boost</th>\n",
       "      <th>Lasso Regression</th>\n",
       "      <th>XG Boost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train R^2 log</th>\n",
       "      <td>0.418725</td>\n",
       "      <td>0.994891</td>\n",
       "      <td>0.949012</td>\n",
       "      <td>0.637676</td>\n",
       "      <td>0.988452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test R^2 log</th>\n",
       "      <td>0.401748</td>\n",
       "      <td>0.964075</td>\n",
       "      <td>0.940312</td>\n",
       "      <td>0.628680</td>\n",
       "      <td>0.964064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train RMSE log</th>\n",
       "      <td>5481.985217</td>\n",
       "      <td>644.387172</td>\n",
       "      <td>1909.857965</td>\n",
       "      <td>4444.689957</td>\n",
       "      <td>852.900651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test RMSE log</th>\n",
       "      <td>5306.539663</td>\n",
       "      <td>1515.519247</td>\n",
       "      <td>2006.700888</td>\n",
       "      <td>4245.152026</td>\n",
       "      <td>1466.117084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        SVR  Random Forest  Gradient Boost  Lasso Regression  \\\n",
       "Train R^2 log      0.418725       0.994891        0.949012          0.637676   \n",
       "Test R^2 log       0.401748       0.964075        0.940312          0.628680   \n",
       "Train RMSE log  5481.985217     644.387172     1909.857965       4444.689957   \n",
       "Test RMSE log   5306.539663    1515.519247     2006.700888       4245.152026   \n",
       "\n",
       "                   XG Boost  \n",
       "Train R^2 log      0.988452  \n",
       "Test R^2 log       0.964064  \n",
       "Train RMSE log   852.900651  \n",
       "Test RMSE log   1466.117084  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_logTarget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best performance so far for Random Forest and XGBoost: XG Boost has R^2 = 0.964 RMSE = 1466, RF R^2 = 0.963 and RMSE = 1515\n",
    "\n",
    "Compare training speed for RF and XG Boost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare training speed for XGBoost and RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest training time: 12.2158 seconds\n",
      "XGBoost training time: 0.2167 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "rf =  RandomForestRegressor()\n",
    "xgb = XGBRegressor()\n",
    "\n",
    "# Measure time for Random Forest training\n",
    "start_time_rf = time.time()\n",
    "rf.fit(X_train, y_train_log)\n",
    "end_time_rf = time.time()\n",
    "rf_training_time = end_time_rf - start_time_rf\n",
    "\n",
    "# Measure time for XGBoost training\n",
    "start_time_xgb = time.time()\n",
    "xgb.fit(X_train, y_train_log)\n",
    "end_time_xgb = time.time()\n",
    "xgb_training_time = end_time_xgb - start_time_xgb\n",
    "\n",
    "# Print the training times\n",
    "print(f\"Random Forest training time: {rf_training_time:.4f} seconds\")\n",
    "print(f\"XGBoost training time: {xgb_training_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both quite fast? XGBoost slightly better performing and quicker to train (even though trees are trained sequencially rather than in parallel, shouldn't this make it slower to train than RF?), try to tune XGBoost to see if I can get even better performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost has lots of params to tune! Can still do gridsearch across these as training time only 0.2 secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best params: {'colsample_bytree': 0.7, 'gamma': 0, 'learning_rate': 0.05, 'max_depth': 7, 'min_child_weight': 2, 'n_estimators': 500, 'subsample': 0.8}\n",
      "Best Parameters: {'colsample_bytree': 0.7, 'gamma': 0, 'learning_rate': 0.05, 'max_depth': 7, 'min_child_weight': 2, 'n_estimators': 500, 'subsample': 0.8}\n",
      "Best Score (RMSE): nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BalintStewart\\AppData\\Local\\Temp\\ipykernel_21968\\485205431.py:33: RuntimeWarning: invalid value encountered in sqrt\n",
      "  best_score_RMSE = np.sqrt(-best_score_MSE)\n"
     ]
    }
   ],
   "source": [
    "# this gridsearch takes a while to run (>15min), don't run it every time we run the notebook\n",
    "run_gridsearch = False\n",
    "\n",
    "xgb = XGBRegressor()\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 300, 500],        # no of boosting rounds (trees), generally 500 upper limit? can get overfitting with more trees\n",
    "    'learning_rate': [0.01, 0.05, 0.1],     \n",
    "    'max_depth': [5, 7, 10],                 # this dataset quite big so deeper tree better?\n",
    "    'subsample': [0.7, 0.8, 1.0],           # fraction of samples used to grow each tree. values between 0.7 and 1 commonly tested\n",
    "    'colsample_bytree': [0.7, 0.8, 1.0],    # fraction of features used to grow each tree \n",
    "    'gamma': [0, 0.1, 0.2],                 # min loss reduction required to make a split\n",
    "    'min_child_weight': [1, 2, 3],          # min number of samples needed in each leaf\n",
    "}\n",
    "if run_gridsearch:\n",
    "   \n",
    "    grid_search = GridSearchCV(estimator = xgb, param_grid= param_grid,\n",
    "                           cv = 3, verbose = 1, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train_log)\n",
    "    \n",
    "    # Save the best parameters to avoid rerunning\n",
    "    with open('best_params.json', 'w') as f:\n",
    "        json.dump(grid_search.best_params_, f)\n",
    "else:\n",
    "    # Load the best parameters if grid search is skipped\n",
    "    with open('best_params.json', 'r') as f:\n",
    "        best_params = json.load(f)\n",
    "    \n",
    "    print(\"Loaded best params:\", best_params)\n",
    "\n",
    "# Get the best parameters and model performance\n",
    "best_params = grid_search.best_params_\n",
    "best_score_MSE = grid_search.best_score_ #this is neg value\n",
    "best_score_RMSE = np.sqrt(-best_score_MSE)\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Score (RMSE): {best_score_RMSE:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'colsample_bytree': 0.7, 'gamma': 0, 'learning_rate': 0.05, 'max_depth': 7, 'min_child_weight': 2, 'n_estimators': 500, 'subsample': 0.8}\n",
      "Best Score (RMSE): 0.9836\n"
     ]
    }
   ],
   "source": [
    "# Get the best parameters and model performance\n",
    "best_params = grid_search.best_params_\n",
    "best_score_MSE = grid_search.best_score_ #this is neg value\n",
    "best_score_MSE = abs(grid_search.best_score_)\n",
    "best_score_RMSE = np.sqrt(best_score_MSE)\n",
    "                  \n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Score (RMSE): {best_score_RMSE:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_depth = 7 was the deepest option in the grid, dataset is quite large so could mean we could improve by increasing this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBRegressor(colsample_bytree = 0.7, gamma = 0, learning_rate = 0.05, max_depth = 7, min_child_weight = 2, n_estimators = 500, subsample = 0.8)\n",
    "\n",
    "xgb_model.fit(X_train, y_train_log)\n",
    "# make predictions on the training data and the test data\n",
    "y_train_pred_log = xgb_model.predict(X_train)\n",
    "y_test_pred_log = xgb_model.predict(X_test)\n",
    "# Inverse log transform the predictions and the true values to original scale, so can compare RMSE values across log and non-transformed target variable\n",
    "y_train_pred_original = np.exp(y_train_pred_log)\n",
    "y_test_pred_original = np.exp(y_test_pred_log)\n",
    "y_train_original = np.exp(y_train_log)\n",
    "y_test_original = np.exp(y_test_log)\n",
    "# Gather evaluation metrics mean_squared_error and r2_score on both train and test datasets\n",
    "train_r2 = r2_score(y_train_log, y_train_pred_log)\n",
    "test_r2 = r2_score(y_test_log, y_test_pred_log)\n",
    "train_MSE = mean_squared_error(y_train_original, y_train_pred_original)\n",
    "test_MSE = mean_squared_error(y_test_original, y_test_pred_original)\n",
    "train_RMSE = np.sqrt(train_MSE)\n",
    "test_RMSE = np.sqrt(test_MSE)\n",
    "\n",
    "results = {\n",
    "        'Train R^2':train_r2,\n",
    "        'Test R^2':test_r2,\n",
    "        'Train RMSE':train_RMSE,\n",
    "        'Test RMSE': test_RMSE\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train R^2</th>\n",
       "      <th>Test R^2</th>\n",
       "      <th>Train RMSE</th>\n",
       "      <th>Test RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.992886</td>\n",
       "      <td>0.969465</td>\n",
       "      <td>659.362743</td>\n",
       "      <td>1384.558161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Train R^2  Test R^2  Train RMSE    Test RMSE\n",
       "XGBoost   0.992886  0.969465  659.362743  1384.558161"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGB_tuned = pd.DataFrame(results, index=['XGBoost'])\n",
    "XGB_tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance is very good but model is still overfitting a little bit. Could decrease learning rate, and/or increase min_child_rate, gains likely to be quite small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best result I can find using XGBoost is R^2 0.969, RMSE = $1385"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some basic plots looking at predictions vs observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at feature importance using in-built xgb.feature_importance()\n",
    "Look at 'gain' first: teh average gain of the feature when it is used in trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance scores for all features, get gain, weight and cover measures\n",
    "gain_importances = xgb_model.get_booster().get_score(importance_type='gain')\n",
    "weight_importances = xgb_model.get_booster().get_score(importance_type='weight')\n",
    "cover_importances = xgb_model.get_booster().get_score(importance_type='cover')\n",
    "\n",
    "\n",
    "# store the dictionaries of raw feature importance values in a df\n",
    "gain_df = pd.DataFrame({\n",
    "    'Feature': list(gain_importances.keys()),\n",
    "    'gain_importance': list(gain_importances.values())\n",
    "})\n",
    "weight_df = pd.DataFrame({\n",
    "    'Feature': list(weight_importances.keys()),\n",
    "    'weight_importance': list(weight_importances.values())\n",
    "})\n",
    "cover_df = pd.DataFrame({\n",
    "    'Feature': list(cover_importances.keys()),\n",
    "    'cover_importance': list(cover_importances.values())\n",
    "})\n",
    "xgb_feature_importance = pd.merge(gain_df, weight_df, on = 'Feature', how ='outer')\n",
    "xgb_feature_importance = pd.merge(xgb_feature_importance, cover_df, on = 'Feature', how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xgb_feature_importance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Have a look at top 20 features by gain importance\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m top20 \u001b[38;5;241m=\u001b[39m \u001b[43mxgb_feature_importance\u001b[49m\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgain_importance\u001b[39m\u001b[38;5;124m'\u001b[39m,ascending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)[:\u001b[38;5;241m20\u001b[39m]\n\u001b[0;32m      3\u001b[0m top20\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'xgb_feature_importance' is not defined"
     ]
    }
   ],
   "source": [
    "# Have a look at top 20 features by gain importance\n",
    "top20 = xgb_feature_importance.sort_values(by= 'gain_importance',ascending = False)[:20]\n",
    "top20.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'top20' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m, figsize \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m18\u001b[39m,\u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# plot the gain_importance\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m sns\u001b[38;5;241m.\u001b[39mbarplot(x \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeature\u001b[39m\u001b[38;5;124m'\u001b[39m, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgain_importance\u001b[39m\u001b[38;5;124m'\u001b[39m, data \u001b[38;5;241m=\u001b[39m \u001b[43mtop20\u001b[49m, ax \u001b[38;5;241m=\u001b[39m axes[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'top20' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABbEAAAH/CAYAAABpW5AvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAqJUlEQVR4nO3db2zW9b3/8XcptJXMVjwcyp9Tx9Ed5zYVHEhXHTEuPWuiYYcbJ+PoAhzi9LhxjKM5Z4J/6Jwb5Tg1JBNHZHpccuaBzahnGaQe1zOyOHtCBjRxR9A4cHCWtcLZoWW4tdJ+fzd21v06Cu1Vvm0/LY9H0htc57raTz+B83JPSluUZVkWAAAAAACQoEljfQAAAAAAADgTERsAAAAAgGSJ2AAAAAAAJEvEBgAAAAAgWSI2AAAAAADJErEBAAAAAEiWiA0AAAAAQLJEbAAAAAAAkiViAwAAAACQLBEbAAAAAIBkFRyxf/SjH8WSJUti9uzZUVRUFC+++OKgr9m1a1d89KMfjdLS0vjABz4QzzzzzDCOCgAMlb0GgPTZawAYmoIj9smTJ2PevHmxefPmIT3/0KFDcfPNN8eNN94Yra2t8YUvfCE++9nPxksvvVTwYQGAobHXAJA+ew0AQ1OUZVk27BcXFcULL7wQS5cuPeNz7rnnntixY0f89Kc/7Xvsb/7mb+L48ePR1NQ03A8NAAyRvQaA9NlrADizySP9AVpaWqK2trbfY3V1dfGFL3zhjK/p6uqKrq6uvl/39vbGr371q/iTP/mTKCoqGqmjAnCeyrIsTpw4EbNnz45Jk87PHxdhrwFInb221wCMDyOx2SMesdva2qKysrLfY5WVldHZ2Rm/+c1v4oILLjjtNY2NjfHggw+O9NEAoJ8jR47En/3Zn431McaEvQZgvLDX9hqA8SHPzR7xiD0c69ati/r6+r5fd3R0xCWXXBJHjhyJ8vLyMTwZABNRZ2dnVFVVxYUXXjjWRxlX7DUAo8leD4+9BmC0jcRmj3jEnjlzZrS3t/d7rL29PcrLywf8W+KIiNLS0igtLT3t8fLyciMLwIg5n/9Jrb0GYLyw1/YagPEhz80e8W8kVlNTE83Nzf0ee/nll6OmpmakPzQAMET2GgDSZ68BOF8VHLF//etfR2tra7S2tkZExKFDh6K1tTUOHz4cEb/7p0orVqzoe/6dd94ZBw8ejC9+8Ytx4MCBeOKJJ+I73/lOrFmzJp/PAAA4jb0GgPTZawAYmoIj9k9+8pO45ppr4pprromIiPr6+rjmmmti/fr1ERHxy1/+sm9wIyL+/M//PHbs2BEvv/xyzJs3Lx599NH45je/GXV1dTl9CgDAH7PXAJA+ew0AQ1OUZVk21ocYTGdnZ1RUVERHR4fv2QVA7uxMPtwjACPJzuTDPQIw0kZia0b8e2IDAAAAAMBwidgAAAAAACRLxAYAAAAAIFkiNgAAAAAAyRKxAQAAAABIlogNAAAAAECyRGwAAAAAAJIlYgMAAAAAkCwRGwAAAACAZInYAAAAAAAkS8QGAAAAACBZIjYAAAAAAMkSsQEAAAAASJaIDQAAAABAskRsAAAAAACSJWIDAAAAAJAsERsAAAAAgGSJ2AAAAAAAJEvEBgAAAAAgWSI2AAAAAADJErEBAAAAAEiWiA0AAAAAQLJEbAAAAAAAkiViAwAAAACQLBEbAAAAAIBkidgAAAAAACRLxAYAAAAAIFkiNgAAAAAAyRKxAQAAAABIlogNAAAAAECyRGwAAAAAAJIlYgMAAAAAkCwRGwAAAACAZInYAAAAAAAkS8QGAAAAACBZIjYAAAAAAMkSsQEAAAAASJaIDQAAAABAskRsAAAAAACSJWIDAAAAAJAsERsAAAAAgGSJ2AAAAAAAJEvEBgAAAAAgWSI2AAAAAADJErEBAAAAAEiWiA0AAAAAQLJEbAAAAAAAkiViAwAAAACQLBEbAAAAAIBkidgAAAAAACRLxAYAAAAAIFkiNgAAAAAAyRKxAQAAAABIlogNAAAAAECyRGwAAAAAAJIlYgMAAAAAkCwRGwAAAACAZInYAAAAAAAkS8QGAAAAACBZIjYAAAAAAMkSsQEAAAAASJaIDQAAAABAskRsAAAAAACSJWIDAAAAAJAsERsAAAAAgGSJ2AAAAAAAJEvEBgAAAAAgWSI2AAAAAADJErEBAAAAAEiWiA0AAAAAQLJEbAAAAAAAkiViAwAAAACQLBEbAAAAAIBkidgAAAAAACRLxAYAAAAAIFkiNgAAAAAAyRKxAQAAAABIlogNAAAAAECyRGwAAAAAAJIlYgMAAAAAkCwRGwAAAACAZA0rYm/evDnmzp0bZWVlUV1dHbt37z7r8zdt2hQf/OAH44ILLoiqqqpYs2ZN/Pa3vx3WgQGAobHXADA+2GwAOLuCI/b27dujvr4+GhoaYu/evTFv3ryoq6uLd955Z8DnP/vss7F27dpoaGiI/fv3x1NPPRXbt2+Pe++995wPDwAMzF4DwPhgswFgcAVH7Mceeyxuv/32WLVqVXz4wx+OLVu2xNSpU+Ppp58e8PmvvvpqXH/99XHrrbfG3Llz45Of/GTccsstg/7NMgAwfPYaAMYHmw0AgysoYnd3d8eePXuitrb2D+9g0qSora2NlpaWAV9z3XXXxZ49e/oG9eDBg7Fz58646aabzuHYAMCZ2GsAGB9sNgAMzeRCnnzs2LHo6emJysrKfo9XVlbGgQMHBnzNrbfeGseOHYuPf/zjkWVZnDp1Ku68886z/lOnrq6u6Orq6vt1Z2dnIccEgPOavQaA8WE0NtteAzARDOsHOxZi165dsWHDhnjiiSdi79698fzzz8eOHTvioYceOuNrGhsbo6Kiou+tqqpqpI8JAOc1ew0A40Ohm22vAZgIirIsy4b65O7u7pg6dWo899xzsXTp0r7HV65cGcePH49/+7d/O+01ixcvjo997GPxta99re+xf/mXf4k77rgjfv3rX8ekSad39IH+priqqio6OjqivLx8qMcFgCHp7OyMioqKCbMz9hqAiWii7XXE6Gy2vQZgtI3EZhf0ldglJSWxYMGCaG5u7nust7c3mpubo6amZsDXvPvuu6eNaHFxcUREnKmfl5aWRnl5eb83AGBo7DUAjA+jsdn2GoCJoKDviR0RUV9fHytXroyFCxfGokWLYtOmTXHy5MlYtWpVRESsWLEi5syZE42NjRERsWTJknjsscfimmuuierq6njrrbfigQceiCVLlvQNLQCQL3sNAOODzQaAwRUcsZctWxZHjx6N9evXR1tbW8yfPz+ampr6fhDF4cOH+/2t8P333x9FRUVx//33xy9+8Yv40z/901iyZEl89atfze+zAAD6sdcAMD7YbAAYXEHfE3usTMTvfQZAOuxMPtwjACPJzuTDPQIw0sb8e2IDAAAAAMBoErEBAAAAAEiWiA0AAAAAQLJEbAAAAAAAkiViAwAAAACQLBEbAAAAAIBkidgAAAAAACRLxAYAAAAAIFkiNgAAAAAAyRKxAQAAAABIlogNAAAAAECyRGwAAAAAAJIlYgMAAAAAkCwRGwAAAACAZInYAAAAAAAkS8QGAAAAACBZIjYAAAAAAMkSsQEAAAAASJaIDQAAAABAskRsAAAAAACSJWIDAAAAAJAsERsAAAAAgGSJ2AAAAAAAJEvEBgAAAAAgWSI2AAAAAADJErEBAAAAAEiWiA0AAAAAQLJEbAAAAAAAkiViAwAAAACQLBEbAAAAAIBkidgAAAAAACRLxAYAAAAAIFkiNgAAAAAAyRKxAQAAAABIlogNAAAAAECyRGwAAAAAAJIlYgMAAAAAkCwRGwAAAACAZInYAAAAAAAkS8QGAAAAACBZIjYAAAAAAMkSsQEAAAAASJaIDQAAAABAskRsAAAAAACSJWIDAAAAAJAsERsAAAAAgGSJ2AAAAAAAJEvEBgAAAAAgWSI2AAAAAADJErEBAAAAAEiWiA0AAAAAQLJEbAAAAAAAkiViAwAAAACQLBEbAAAAAIBkidgAAAAAACRLxAYAAAAAIFkiNgAAAAAAyRKxAQAAAABIlogNAAAAAECyRGwAAAAAAJIlYgMAAAAAkCwRGwAAAACAZInYAAAAAAAkS8QGAAAAACBZIjYAAAAAAMkSsQEAAAAASJaIDQAAAABAskRsAAAAAACSJWIDAAAAAJAsERsAAAAAgGSJ2AAAAAAAJEvEBgAAAAAgWSI2AAAAAADJErEBAAAAAEiWiA0AAAAAQLJEbAAAAAAAkiViAwAAAACQLBEbAAAAAIBkidgAAAAAACRLxAYAAAAAIFkiNgAAAAAAyRKxAQAAAABI1rAi9ubNm2Pu3LlRVlYW1dXVsXv37rM+//jx47F69eqYNWtWlJaWxuWXXx47d+4c1oEBgKGx1wAwPthsADi7yYW+YPv27VFfXx9btmyJ6urq2LRpU9TV1cUbb7wRM2bMOO353d3d8Zd/+ZcxY8aMeO6552LOnDnx85//PC666KI8zg8ADMBeA8D4YLMBYHBFWZZlhbyguro6rr322nj88ccjIqK3tzeqqqrirrvuirVr1572/C1btsTXvva1OHDgQEyZMmVYh+zs7IyKioro6OiI8vLyYb0PADiTibgz9hqAiWai7sxob/ZEvUcA0jESW1PQtxPp7u6OPXv2RG1t7R/ewaRJUVtbGy0tLQO+5nvf+17U1NTE6tWro7KyMq688srYsGFD9PT0nPHjdHV1RWdnZ783AGBo7DUAjA+jsdn2GoCJoKCIfezYsejp6YnKysp+j1dWVkZbW9uArzl48GA899xz0dPTEzt37owHHnggHn300fjKV75yxo/T2NgYFRUVfW9VVVWFHBMAzmv2GgDGh9HYbHsNwEQwrB/sWIje3t6YMWNGPPnkk7FgwYJYtmxZ3HfffbFly5YzvmbdunXR0dHR93bkyJGRPiYAnNfsNQCMD4Vutr0GYCIo6Ac7Tp8+PYqLi6O9vb3f4+3t7TFz5swBXzNr1qyYMmVKFBcX9z32oQ99KNra2qK7uztKSkpOe01paWmUlpYWcjQA4P/YawAYH0Zjs+01ABNBQV+JXVJSEgsWLIjm5ua+x3p7e6O5uTlqamoGfM31118fb731VvT29vY99uabb8asWbMG/B/EAMC5sdcAMD7YbAAYmoK/nUh9fX1s3bo1vvWtb8X+/fvjc5/7XJw8eTJWrVoVERErVqyIdevW9T3/c5/7XPzqV7+Ku+++O958883YsWNHbNiwIVavXp3fZwEA9GOvAWB8sNkAMLiCvp1IRMSyZcvi6NGjsX79+mhra4v58+dHU1NT3w+iOHz4cEya9Ic2XlVVFS+99FKsWbMmrr766pgzZ07cfffdcc899+T3WQAA/dhrABgfbDYADK4oy7JsrA8xmM7OzqioqIiOjo4oLy8f6+MAMMHYmXy4RwBGkp3Jh3sEYKSNxNYU/O1EAAAAAABgtIjYAAAAAAAkS8QGAAAAACBZIjYAAAAAAMkSsQEAAAAASJaIDQAAAABAskRsAAAAAACSJWIDAAAAAJAsERsAAAAAgGSJ2AAAAAAAJEvEBgAAAAAgWSI2AAAAAADJErEBAAAAAEiWiA0AAAAAQLJEbAAAAAAAkiViAwAAAACQLBEbAAAAAIBkidgAAAAAACRLxAYAAAAAIFkiNgAAAAAAyRKxAQAAAABIlogNAAAAAECyRGwAAAAAAJIlYgMAAAAAkCwRGwAAAACAZInYAAAAAAAkS8QGAAAAACBZIjYAAAAAAMkSsQEAAAAASJaIDQAAAABAskRsAAAAAACSJWIDAAAAAJAsERsAAAAAgGSJ2AAAAAAAJEvEBgAAAAAgWSI2AAAAAADJErEBAAAAAEiWiA0AAAAAQLJEbAAAAAAAkiViAwAAAACQLBEbAAAAAIBkidgAAAAAACRLxAYAAAAAIFkiNgAAAAAAyRKxAQAAAABIlogNAAAAAECyRGwAAAAAAJIlYgMAAAAAkCwRGwAAAACAZInYAAAAAAAkS8QGAAAAACBZIjYAAAAAAMkSsQEAAAAASJaIDQAAAABAskRsAAAAAACSJWIDAAAAAJAsERsAAAAAgGSJ2AAAAAAAJEvEBgAAAAAgWSI2AAAAAADJErEBAAAAAEiWiA0AAAAAQLJEbAAAAAAAkiViAwAAAACQLBEbAAAAAIBkidgAAAAAACRLxAYAAAAAIFkiNgAAAAAAyRKxAQAAAABIlogNAAAAAECyRGwAAAAAAJIlYgMAAAAAkCwRGwAAAACAZInYAAAAAAAkS8QGAAAAACBZIjYAAAAAAMkSsQEAAAAASJaIDQAAAABAskRsAAAAAACSJWIDAAAAAJAsERsAAAAAgGQNK2Jv3rw55s6dG2VlZVFdXR27d+8e0uu2bdsWRUVFsXTp0uF8WACgAPYaAMYHmw0AZ1dwxN6+fXvU19dHQ0ND7N27N+bNmxd1dXXxzjvvnPV1b7/9dvzDP/xDLF68eNiHBQCGxl4DwPhgswFgcAVH7Mceeyxuv/32WLVqVXz4wx+OLVu2xNSpU+Ppp58+42t6enriM5/5TDz44INx6aWXntOBAYDB2WsAGB9sNgAMrqCI3d3dHXv27Ina2to/vINJk6K2tjZaWlrO+Lovf/nLMWPGjLjtttuG9HG6urqis7Oz3xsAMDT2GgDGh9HYbHsNwERQUMQ+duxY9PT0RGVlZb/HKysro62tbcDXvPLKK/HUU0/F1q1bh/xxGhsbo6Kiou+tqqqqkGMCwHnNXgPA+DAam22vAZgIhvWDHYfqxIkTsXz58ti6dWtMnz59yK9bt25ddHR09L0dOXJkBE8JAOc3ew0A48NwNtteAzARTC7kydOnT4/i4uJob2/v93h7e3vMnDnztOf/7Gc/i7fffjuWLFnS91hvb+/vPvDkyfHGG2/EZZdddtrrSktLo7S0tJCjAQD/x14DwPgwGpttrwGYCAr6SuySkpJYsGBBNDc39z3W29sbzc3NUVNTc9rzr7jiinjttdeitbW17+1Tn/pU3HjjjdHa2uqfMQHACLDXADA+2GwAGJqCvhI7IqK+vj5WrlwZCxcujEWLFsWmTZvi5MmTsWrVqoiIWLFiRcyZMycaGxujrKwsrrzyyn6vv+iiiyIiTnscAMiPvQaA8cFmA8DgCo7Yy5Yti6NHj8b69eujra0t5s+fH01NTX0/iOLw4cMxadKIfqttAGAQ9hoAxgebDQCDK8qyLBvrQwyms7MzKioqoqOjI8rLy8f6OABMMHYmH+4RgJFkZ/LhHgEYaSOxNf46FwAAAACAZInYAAAAAAAkS8QGAAAAACBZIjYAAAAAAMkSsQEAAAAASJaIDQAAAABAskRsAAAAAACSJWIDAAAAAJAsERsAAAAAgGSJ2AAAAAAAJEvEBgAAAAAgWSI2AAAAAADJErEBAAAAAEiWiA0AAAAAQLJEbAAAAAAAkiViAwAAAACQLBEbAAAAAIBkidgAAAAAACRLxAYAAAAAIFkiNgAAAAAAyRKxAQAAAABIlogNAAAAAECyRGwAAAAAAJIlYgMAAAAAkCwRGwAAAACAZInYAAAAAAAkS8QGAAAAACBZIjYAAAAAAMkSsQEAAAAASJaIDQAAAABAskRsAAAAAACSJWIDAAAAAJAsERsAAAAAgGSJ2AAAAAAAJEvEBgAAAAAgWSI2AAAAAADJErEBAAAAAEiWiA0AAAAAQLJEbAAAAAAAkiViAwAAAACQLBEbAAAAAIBkidgAAAAAACRLxAYAAAAAIFkiNgAAAAAAyRKxAQAAAABIlogNAAAAAECyRGwAAAAAAJIlYgMAAAAAkCwRGwAAAACAZInYAAAAAAAkS8QGAAAAACBZIjYAAAAAAMkSsQEAAAAASJaIDQAAAABAskRsAAAAAACSJWIDAAAAAJAsERsAAAAAgGSJ2AAAAAAAJEvEBgAAAAAgWSI2AAAAAADJErEBAAAAAEiWiA0AAAAAQLJEbAAAAAAAkiViAwAAAACQLBEbAAAAAIBkidgAAAAAACRLxAYAAAAAIFkiNgAAAAAAyRKxAQAAAABIlogNAAAAAECyRGwAAAAAAJIlYgMAAAAAkCwRGwAAAACAZInYAAAAAAAkS8QGAAAAACBZIjYAAAAAAMkSsQEAAAAASJaIDQAAAABAskRsAAAAAACSJWIDAAAAAJAsERsAAAAAgGSJ2AAAAAAAJGtYEXvz5s0xd+7cKCsri+rq6ti9e/cZn7t169ZYvHhxTJs2LaZNmxa1tbVnfT4AkA97DQDjg80GgLMrOGJv37496uvro6GhIfbu3Rvz5s2Lurq6eOeddwZ8/q5du+KWW26JH/7wh9HS0hJVVVXxyU9+Mn7xi1+c8+EBgIHZawAYH2w2AAyuKMuyrJAXVFdXx7XXXhuPP/54RET09vZGVVVV3HXXXbF27dpBX9/T0xPTpk2Lxx9/PFasWDGkj9nZ2RkVFRXR0dER5eXlhRwXAAY1EXfGXgMw0UzUnRntzZ6o9whAOkZiawr6Suzu7u7Ys2dP1NbW/uEdTJoUtbW10dLSMqT38e6778Z7770XF1988Rmf09XVFZ2dnf3eAIChsdcAMD6MxmbbawAmgoIi9rFjx6KnpycqKyv7PV5ZWRltbW1Deh/33HNPzJ49u99I/7HGxsaoqKjoe6uqqirkmABwXrPXADA+jMZm22sAJoJh/WDH4dq4cWNs27YtXnjhhSgrKzvj89atWxcdHR19b0eOHBnFUwLA+c1eA8D4MJTNttcATASTC3ny9OnTo7i4ONrb2/s93t7eHjNnzjzrax955JHYuHFj/OAHP4irr776rM8tLS2N0tLSQo4GAPwfew0A48NobLa9BmAiKOgrsUtKSmLBggXR3Nzc91hvb280NzdHTU3NGV/38MMPx0MPPRRNTU2xcOHC4Z8WABiUvQaA8cFmA8DQFPSV2BER9fX1sXLlyli4cGEsWrQoNm3aFCdPnoxVq1ZFRMSKFStizpw50djYGBER//RP/xTr16+PZ599NubOndv3fb3e9773xfve974cPxUA4PfsNQCMDzYbAAZXcMRetmxZHD16NNavXx9tbW0xf/78aGpq6vtBFIcPH45Jk/7wBd7f+MY3oru7O/76r/+63/tpaGiIL33pS+d2egBgQPYaAMYHmw0AgyvKsiwb60MMprOzMyoqKqKjoyPKy8vH+jgATDB2Jh/uEYCRZGfy4R4BGGkjsTUFfU9sAAAAAAAYTSI2AAAAAADJErEBAAAAAEiWiA0AAAAAQLJEbAAAAAAAkiViAwAAAACQLBEbAAAAAIBkidgAAAAAACRLxAYAAAAAIFkiNgAAAAAAyRKxAQAAAABIlogNAAAAAECyRGwAAAAAAJIlYgMAAAAAkCwRGwAAAACAZInYAAAAAAAkS8QGAAAAACBZIjYAAAAAAMkSsQEAAAAASJaIDQAAAABAskRsAAAAAACSJWIDAAAAAJAsERsAAAAAgGSJ2AAAAAAAJEvEBgAAAAAgWSI2AAAAAADJErEBAAAAAEiWiA0AAAAAQLJEbAAAAAAAkiViAwAAAACQLBEbAAAAAIBkidgAAAAAACRLxAYAAAAAIFkiNgAAAAAAyRKxAQAAAABIlogNAAAAAECyRGwAAAAAAJIlYgMAAAAAkCwRGwAAAACAZInYAAAAAAAkS8QGAAAAACBZIjYAAAAAAMkSsQEAAAAASJaIDQAAAABAskRsAAAAAACSJWIDAAAAAJAsERsAAAAAgGSJ2AAAAAAAJEvEBgAAAAAgWSI2AAAAAADJErEBAAAAAEiWiA0AAAAAQLJEbAAAAAAAkiViAwAAAACQLBEbAAAAAIBkidgAAAAAACRLxAYAAAAAIFkiNgAAAAAAyRKxAQAAAABIlogNAAAAAECyRGwAAAAAAJIlYgMAAAAAkCwRGwAAAACAZInYAAAAAAAkS8QGAAAAACBZIjYAAAAAAMkSsQEAAAAASJaIDQAAAABAskRsAAAAAACSJWIDAAAAAJAsERsAAAAAgGSJ2AAAAAAAJEvEBgAAAAAgWSI2AAAAAADJErEBAAAAAEiWiA0AAAAAQLJEbAAAAAAAkiViAwAAAACQLBEbAAAAAIBkidgAAAAAACRLxAYAAAAAIFnDitibN2+OuXPnRllZWVRXV8fu3bvP+vzvfve7ccUVV0RZWVlcddVVsXPnzmEdFgAYOnsNAOODzQaAsys4Ym/fvj3q6+ujoaEh9u7dG/PmzYu6urp45513Bnz+q6++GrfcckvcdtttsW/fvli6dGksXbo0fvrTn57z4QGAgdlrABgfbDYADK4oy7KskBdUV1fHtddeG48//nhERPT29kZVVVXcddddsXbt2tOev2zZsjh58mR8//vf73vsYx/7WMyfPz+2bNkypI/Z2dkZFRUV0dHREeXl5YUcFwAGNRF3xl4DMNFM1J0Z7c2eqPcIQDpGYmsmF/Lk7u7u2LNnT6xbt67vsUmTJkVtbW20tLQM+JqWlpaor6/v91hdXV28+OKLZ/w4XV1d0dXV1ffrjo6OiPjdBQBA3n6/LwX+vW6y7DUAE9FE2+uI0dlsew3AaBuJzS4oYh87dix6enqisrKy3+OVlZVx4MCBAV/T1tY24PPb2trO+HEaGxvjwQcfPO3xqqqqQo4LAAX5n//5n6ioqBjrY5wzew3ARDZR9jpidDbbXgMwVvLc7IIi9mhZt25dv79ZPn78eLz//e+Pw4cPT5j/WBkLnZ2dUVVVFUeOHPHPxs6Be8yHe8yHe8xHR0dHXHLJJXHxxReP9VHGFXs9Mvy5zod7zId7zId7zIe9Hh57PTL8uc6Pu8yHe8yHe8zHSGx2QRF7+vTpUVxcHO3t7f0eb29vj5kzZw74mpkzZxb0/IiI0tLSKC0tPe3xiooKv4FyUF5e7h5z4B7z4R7z4R7zMWlSwT/vOEn2emLw5zof7jEf7jEf7jEfE2WvI0Zns+31yPLnOj/uMh/uMR/uMR95bnZB76mkpCQWLFgQzc3NfY/19vZGc3Nz1NTUDPiampqafs+PiHj55ZfP+HwA4NzYawAYH2w2AAxNwd9OpL6+PlauXBkLFy6MRYsWxaZNm+LkyZOxatWqiIhYsWJFzJkzJxobGyMi4u67744bbrghHn300bj55ptj27Zt8ZOf/CSefPLJfD8TAKCPvQaA8cFmA8DgCo7Yy5Yti6NHj8b69eujra0t5s+fH01NTX0/WOLw4cP9vlT8uuuui2effTbuv//+uPfee+Mv/uIv4sUXX4wrr7xyyB+ztLQ0GhoaBvwnUAyde8yHe8yHe8yHe8zHRLxHez1+ucd8uMd8uMd8uMd8TNR7HO3Nnqj3ONrcY37cZT7cYz7cYz5G4h6LsizLcntvAAAAAACQo4nzEzEAAAAAAJhwRGwAAAAAAJIlYgMAAAAAkCwRGwAAAACAZCUTsTdv3hxz586NsrKyqK6ujt27d5/1+d/97nfjiiuuiLKysrjqqqti586do3TStBVyj1u3bo3FixfHtGnTYtq0aVFbWzvovZ8vCv39+Hvbtm2LoqKiWLp06cgecJwo9B6PHz8eq1evjlmzZkVpaWlcfvnl/mxH4fe4adOm+OAHPxgXXHBBVFVVxZo1a+K3v/3tKJ02TT/60Y9iyZIlMXv27CgqKooXX3xx0Nfs2rUrPvrRj0ZpaWl84AMfiGeeeWbEzzke2Ot82Ot82Ot82Ot82OtzZ6/zY6/zYa/zYa/zYa/zY7PPzZjtdZaAbdu2ZSUlJdnTTz+d/dd//Vd2++23ZxdddFHW3t4+4PN//OMfZ8XFxdnDDz+cvf7669n999+fTZkyJXvttddG+eRpKfQeb7311mzz5s3Zvn37sv3792d/+7d/m1VUVGT//d//PconT0uh9/h7hw4dyubMmZMtXrw4+6u/+qvROWzCCr3Hrq6ubOHChdlNN92UvfLKK9mhQ4eyXbt2Za2traN88rQUeo/f/va3s9LS0uzb3/52dujQoeyll17KZs2ala1Zs2aUT56WnTt3Zvfdd1/2/PPPZxGRvfDCC2d9/sGDB7OpU6dm9fX12euvv559/etfz4qLi7OmpqbROXCi7HU+7HU+7HU+7HU+7HU+7HU+7HU+7HU+7HU+7HV+bPa5G6u9TiJiL1q0KFu9enXfr3t6erLZs2dnjY2NAz7/05/+dHbzzTf3e6y6ujr7u7/7uxE9Z+oKvcc/durUqezCCy/MvvWtb43UEceF4dzjqVOnsuuuuy775je/ma1cudLIZoXf4ze+8Y3s0ksvzbq7u0friONCofe4evXq7BOf+ES/x+rr67Prr79+RM85ngxlZL/4xS9mH/nIR/o9tmzZsqyurm4ET5Y+e50Pe50Pe50Pe50Pe50/ez189jof9jof9jof9jo/Njtfo7nXY/7tRLq7u2PPnj1RW1vb99ikSZOitrY2WlpaBnxNS0tLv+dHRNTV1Z3x+eeD4dzjH3v33Xfjvffei4svvnikjpm84d7jl7/85ZgxY0bcdttto3HM5A3nHr/3ve9FTU1NrF69OiorK+PKK6+MDRs2RE9Pz2gdOznDucfrrrsu9uzZ0/fPoQ4ePBg7d+6Mm266aVTOPFHYmdPZ63zY63zY63zY63zY67FjZ05nr/Nhr/Nhr/Nhr/Njs8dGXjszOc9DDcexY8eip6cnKisr+z1eWVkZBw4cGPA1bW1tAz6/ra1txM6ZuuHc4x+75557Yvbs2af9xjqfDOceX3nllXjqqaeitbV1FE44PgznHg8ePBj/8R//EZ/5zGdi586d8dZbb8XnP//5eO+996KhoWE0jp2c4dzjrbfeGseOHYuPf/zjkWVZnDp1Ku6888649957R+PIE8aZdqazszN+85vfxAUXXDBGJxs79jof9jof9jof9jof9nrs2OvT2et82Ot82Ot82Ov82Oyxkddej/lXYpOGjRs3xrZt2+KFF16IsrKysT7OuHHixIlYvnx5bN26NaZPnz7WxxnXent7Y8aMGfHkk0/GggULYtmyZXHffffFli1bxvpo48quXbtiw4YN8cQTT8TevXvj+eefjx07dsRDDz001kcDcmCvh8de58de58New8Rmr4fHXufHXufHZqdjzL8Se/r06VFcXBzt7e39Hm9vb4+ZM2cO+JqZM2cW9PzzwXDu8fceeeSR2LhxY/zgBz+Iq6++eiSPmbxC7/FnP/tZvP3227FkyZK+x3p7eyMiYvLkyfHGG2/EZZddNrKHTtBwfj/OmjUrpkyZEsXFxX2PfehDH4q2trbo7u6OkpKSET1zioZzjw888EAsX748PvvZz0ZExFVXXRUnT56MO+64I+67776YNMnfXQ7FmXamvLz8vPyqrgh7nRd7nQ97nQ97nQ97PXbs9ensdT7sdT7sdT7sdX5s9tjIa6/H/KZLSkpiwYIF0dzc3PdYb29vNDc3R01NzYCvqamp6ff8iIiXX375jM8/HwznHiMiHn744XjooYeiqakpFi5cOBpHTVqh93jFFVfEa6+9Fq2trX1vn/rUp+LGG2+M1tbWqKqqGs3jJ2M4vx+vv/76eOutt/r+IyUi4s0334xZs2adtwM7nHt89913TxvR3/+Hy+9+5gJDYWdOZ6/zYa/zYa/zYa/zYa/Hjp05nb3Oh73Oh73Oh73Oj80eG7ntTEE/BnKEbNu2LSstLc2eeeaZ7PXXX8/uuOOO7KKLLsra2tqyLMuy5cuXZ2vXru17/o9//ONs8uTJ2SOPPJLt378/a2hoyKZMmZK99tprY/UpJKHQe9y4cWNWUlKSPffcc9kvf/nLvrcTJ06M1aeQhELv8Y/56cm/U+g9Hj58OLvwwguzv//7v8/eeOON7Pvf/342Y8aM7Ctf+cpYfQpJKPQeGxoasgsvvDD713/91+zgwYPZv//7v2eXXXZZ9ulPf3qsPoUknDhxItu3b1+2b9++LCKyxx57LNu3b1/285//PMuyLFu7dm22fPnyvucfPHgwmzp1avaP//iP2f79+7PNmzdnxcXFWVNT01h9Ckmw1/mw1/mw1/mw1/mw1/mw1/mw1/mw1/mw1/mw1/mx2edurPY6iYidZVn29a9/PbvkkkuykpKSbNGiRdl//ud/9v3fbrjhhmzlypX9nv+d73wnu/zyy7OSkpLsIx/5SLZjx45RPnGaCrnH97///VlEnPbW0NAw+gdPTKG/H/9/RvYPCr3HV199Nauurs5KS0uzSy+9NPvqV7+anTp1apRPnZ5C7vG9997LvvSlL2WXXXZZVlZWllVVVWWf//zns//93/8d/YMn5Ic//OGA///u93e3cuXK7IYbbjjtNfPnz89KSkqySy+9NPvnf/7nUT93iux1Pux1Pux1Pux1Puz1ubPX+bHX+bDX+bDX+bDX+bHZ52as9rooy3ztOwAAAAAAaRrz74kNAAAAAABnImIDAAAAAJAsERsAAAAAgGSJ2AAAAAAAJEvEBgAAAAAgWSI2AAAAAADJErEBAAAAAEiWiA0AAAAAQLJEbAAAAAAAkiViAwAAAACQLBEbAAAAAIBkidgAAAAAACTr/wG1l0kXjUEQyAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1800x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot barplots of the three metrics, sorted by gain_importance\n",
    "fig, axes = plt.subplots(1,3, figsize = (18,6))\n",
    "\n",
    "# plot the gain_importance\n",
    "sns.barplot(x = 'Feature', y = 'gain_importance', data = top20, ax = axes[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>hp</td>\n",
       "      <td>4.488245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>make_model_Renault Espace</td>\n",
       "      <td>3.024802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>gears</td>\n",
       "      <td>2.138797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>age</td>\n",
       "      <td>1.890445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>make_model_Audi A3</td>\n",
       "      <td>1.491278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Feature  Importance\n",
       "87                         hp    4.488245\n",
       "99  make_model_Renault Espace    3.024802\n",
       "85                      gears    2.138797\n",
       "79                        age    1.890445\n",
       "93         make_model_Audi A3    1.491278"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top20_features.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
